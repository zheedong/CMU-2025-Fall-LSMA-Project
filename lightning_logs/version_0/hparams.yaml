config:
  lm_model_name: meta-llama/Meta-Llama-3-8B-Instruct
  lr: 0.0001
  weight_decay: 0.01
  warmup_steps: 100
  vision_hidden_size: 2048
  freeze_vision: false
  freeze_lm: false
  use_lora: true
  vision_layer: -1
  num_workers: 8
  shuffle_buffer: 10000
  resampled: false
  max_length: 32
  training_epochs: 1
  batch_size: 16
  devices: 2
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  num_sanity_val_steps: 2
  val_check_interval: 1000
  num_training_step: 103437
