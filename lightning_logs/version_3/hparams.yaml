config:
  lm_model_name: meta-llama/Meta-Llama-3-8B-Instruct
  lr: 0.0001
  weight_decay: 0.01
  warmup_steps: 100
  vision_hidden_size: 1024
  freeze_vision: false
  freeze_lm: false
  use_lora: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
